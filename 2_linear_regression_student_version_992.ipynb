{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_linear_regression_student_version-992.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4lxwxXs39hU",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression on the World Population"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1xqVmk039hV",
        "colab_type": "text"
      },
      "source": [
        "Now that we know about cleaning and exploring a dataset, we will now train a simple linear regression model on a set of data. We'll use the world population data from the Analyse Supplementary Exam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOdRT98Q39hW",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY0CRbmN39hX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-AEiZ5d39hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_pop = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/AnalyseProject/world_population.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjolyAA_39he",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "21271258-f069-48fc-ba0d-501bbfd512fb"
      },
      "source": [
        "df_pop.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country Code</th>\n",
              "      <th>1960</th>\n",
              "      <th>1961</th>\n",
              "      <th>1962</th>\n",
              "      <th>1963</th>\n",
              "      <th>1964</th>\n",
              "      <th>1965</th>\n",
              "      <th>1966</th>\n",
              "      <th>1967</th>\n",
              "      <th>1968</th>\n",
              "      <th>1969</th>\n",
              "      <th>1970</th>\n",
              "      <th>1971</th>\n",
              "      <th>1972</th>\n",
              "      <th>1973</th>\n",
              "      <th>1974</th>\n",
              "      <th>1975</th>\n",
              "      <th>1976</th>\n",
              "      <th>1977</th>\n",
              "      <th>1978</th>\n",
              "      <th>1979</th>\n",
              "      <th>1980</th>\n",
              "      <th>1981</th>\n",
              "      <th>1982</th>\n",
              "      <th>1983</th>\n",
              "      <th>1984</th>\n",
              "      <th>1985</th>\n",
              "      <th>1986</th>\n",
              "      <th>1987</th>\n",
              "      <th>1988</th>\n",
              "      <th>1989</th>\n",
              "      <th>1990</th>\n",
              "      <th>1991</th>\n",
              "      <th>1992</th>\n",
              "      <th>1993</th>\n",
              "      <th>1994</th>\n",
              "      <th>1995</th>\n",
              "      <th>1996</th>\n",
              "      <th>1997</th>\n",
              "      <th>1998</th>\n",
              "      <th>1999</th>\n",
              "      <th>2000</th>\n",
              "      <th>2001</th>\n",
              "      <th>2002</th>\n",
              "      <th>2003</th>\n",
              "      <th>2004</th>\n",
              "      <th>2005</th>\n",
              "      <th>2006</th>\n",
              "      <th>2007</th>\n",
              "      <th>2008</th>\n",
              "      <th>2009</th>\n",
              "      <th>2010</th>\n",
              "      <th>2011</th>\n",
              "      <th>2012</th>\n",
              "      <th>2013</th>\n",
              "      <th>2014</th>\n",
              "      <th>2015</th>\n",
              "      <th>2016</th>\n",
              "      <th>2017</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABW</td>\n",
              "      <td>54211.0</td>\n",
              "      <td>55438.0</td>\n",
              "      <td>56225.0</td>\n",
              "      <td>56695.0</td>\n",
              "      <td>57032.0</td>\n",
              "      <td>57360.0</td>\n",
              "      <td>57715.0</td>\n",
              "      <td>58055.0</td>\n",
              "      <td>58386.0</td>\n",
              "      <td>58726.0</td>\n",
              "      <td>59063.0</td>\n",
              "      <td>59440.0</td>\n",
              "      <td>59840.0</td>\n",
              "      <td>60243.0</td>\n",
              "      <td>60528.0</td>\n",
              "      <td>60657.0</td>\n",
              "      <td>60586.0</td>\n",
              "      <td>60366.0</td>\n",
              "      <td>60103.0</td>\n",
              "      <td>59980.0</td>\n",
              "      <td>60096.0</td>\n",
              "      <td>60567.0</td>\n",
              "      <td>61345.0</td>\n",
              "      <td>62201.0</td>\n",
              "      <td>62836.0</td>\n",
              "      <td>63026.0</td>\n",
              "      <td>62644.0</td>\n",
              "      <td>61833.0</td>\n",
              "      <td>61079.0</td>\n",
              "      <td>61032.0</td>\n",
              "      <td>62149.0</td>\n",
              "      <td>64622.0</td>\n",
              "      <td>68235.0</td>\n",
              "      <td>72504.0</td>\n",
              "      <td>76700.0</td>\n",
              "      <td>80324.0</td>\n",
              "      <td>83200.0</td>\n",
              "      <td>85451.0</td>\n",
              "      <td>87277.0</td>\n",
              "      <td>89005.0</td>\n",
              "      <td>90853.0</td>\n",
              "      <td>92898.0</td>\n",
              "      <td>94992.0</td>\n",
              "      <td>97017.0</td>\n",
              "      <td>98737.0</td>\n",
              "      <td>100031.0</td>\n",
              "      <td>100832.0</td>\n",
              "      <td>101220.0</td>\n",
              "      <td>101353.0</td>\n",
              "      <td>101453.0</td>\n",
              "      <td>101669.0</td>\n",
              "      <td>102053.0</td>\n",
              "      <td>102577.0</td>\n",
              "      <td>103187.0</td>\n",
              "      <td>103795.0</td>\n",
              "      <td>104341.0</td>\n",
              "      <td>104822.0</td>\n",
              "      <td>105264.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AFG</td>\n",
              "      <td>8996351.0</td>\n",
              "      <td>9166764.0</td>\n",
              "      <td>9345868.0</td>\n",
              "      <td>9533954.0</td>\n",
              "      <td>9731361.0</td>\n",
              "      <td>9938414.0</td>\n",
              "      <td>10152331.0</td>\n",
              "      <td>10372630.0</td>\n",
              "      <td>10604346.0</td>\n",
              "      <td>10854428.0</td>\n",
              "      <td>11126123.0</td>\n",
              "      <td>11417825.0</td>\n",
              "      <td>11721940.0</td>\n",
              "      <td>12027822.0</td>\n",
              "      <td>12321541.0</td>\n",
              "      <td>12590286.0</td>\n",
              "      <td>12840299.0</td>\n",
              "      <td>13067538.0</td>\n",
              "      <td>13237734.0</td>\n",
              "      <td>13306695.0</td>\n",
              "      <td>13248370.0</td>\n",
              "      <td>13053954.0</td>\n",
              "      <td>12749645.0</td>\n",
              "      <td>12389269.0</td>\n",
              "      <td>12047115.0</td>\n",
              "      <td>11783050.0</td>\n",
              "      <td>11601041.0</td>\n",
              "      <td>11502761.0</td>\n",
              "      <td>11540888.0</td>\n",
              "      <td>11777609.0</td>\n",
              "      <td>12249114.0</td>\n",
              "      <td>12993657.0</td>\n",
              "      <td>13981231.0</td>\n",
              "      <td>15095099.0</td>\n",
              "      <td>16172719.0</td>\n",
              "      <td>17099541.0</td>\n",
              "      <td>17822884.0</td>\n",
              "      <td>18381605.0</td>\n",
              "      <td>18863999.0</td>\n",
              "      <td>19403676.0</td>\n",
              "      <td>20093756.0</td>\n",
              "      <td>20966463.0</td>\n",
              "      <td>21979923.0</td>\n",
              "      <td>23064851.0</td>\n",
              "      <td>24118979.0</td>\n",
              "      <td>25070798.0</td>\n",
              "      <td>25893450.0</td>\n",
              "      <td>26616792.0</td>\n",
              "      <td>27294031.0</td>\n",
              "      <td>28004331.0</td>\n",
              "      <td>28803167.0</td>\n",
              "      <td>29708599.0</td>\n",
              "      <td>30696958.0</td>\n",
              "      <td>31731688.0</td>\n",
              "      <td>32758020.0</td>\n",
              "      <td>33736494.0</td>\n",
              "      <td>34656032.0</td>\n",
              "      <td>35530081.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AGO</td>\n",
              "      <td>5643182.0</td>\n",
              "      <td>5753024.0</td>\n",
              "      <td>5866061.0</td>\n",
              "      <td>5980417.0</td>\n",
              "      <td>6093321.0</td>\n",
              "      <td>6203299.0</td>\n",
              "      <td>6309770.0</td>\n",
              "      <td>6414995.0</td>\n",
              "      <td>6523791.0</td>\n",
              "      <td>6642632.0</td>\n",
              "      <td>6776381.0</td>\n",
              "      <td>6927269.0</td>\n",
              "      <td>7094834.0</td>\n",
              "      <td>7277960.0</td>\n",
              "      <td>7474338.0</td>\n",
              "      <td>7682479.0</td>\n",
              "      <td>7900997.0</td>\n",
              "      <td>8130988.0</td>\n",
              "      <td>8376147.0</td>\n",
              "      <td>8641521.0</td>\n",
              "      <td>8929900.0</td>\n",
              "      <td>9244507.0</td>\n",
              "      <td>9582156.0</td>\n",
              "      <td>9931562.0</td>\n",
              "      <td>10277321.0</td>\n",
              "      <td>10609042.0</td>\n",
              "      <td>10921037.0</td>\n",
              "      <td>11218268.0</td>\n",
              "      <td>11513968.0</td>\n",
              "      <td>11827237.0</td>\n",
              "      <td>12171441.0</td>\n",
              "      <td>12553446.0</td>\n",
              "      <td>12968345.0</td>\n",
              "      <td>13403734.0</td>\n",
              "      <td>13841301.0</td>\n",
              "      <td>14268994.0</td>\n",
              "      <td>14682284.0</td>\n",
              "      <td>15088981.0</td>\n",
              "      <td>15504318.0</td>\n",
              "      <td>15949766.0</td>\n",
              "      <td>16440924.0</td>\n",
              "      <td>16983266.0</td>\n",
              "      <td>17572649.0</td>\n",
              "      <td>18203369.0</td>\n",
              "      <td>18865716.0</td>\n",
              "      <td>19552542.0</td>\n",
              "      <td>20262399.0</td>\n",
              "      <td>20997687.0</td>\n",
              "      <td>21759420.0</td>\n",
              "      <td>22549547.0</td>\n",
              "      <td>23369131.0</td>\n",
              "      <td>24218565.0</td>\n",
              "      <td>25096150.0</td>\n",
              "      <td>25998340.0</td>\n",
              "      <td>26920466.0</td>\n",
              "      <td>27859305.0</td>\n",
              "      <td>28813463.0</td>\n",
              "      <td>29784193.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ALB</td>\n",
              "      <td>1608800.0</td>\n",
              "      <td>1659800.0</td>\n",
              "      <td>1711319.0</td>\n",
              "      <td>1762621.0</td>\n",
              "      <td>1814135.0</td>\n",
              "      <td>1864791.0</td>\n",
              "      <td>1914573.0</td>\n",
              "      <td>1965598.0</td>\n",
              "      <td>2022272.0</td>\n",
              "      <td>2081695.0</td>\n",
              "      <td>2135479.0</td>\n",
              "      <td>2187853.0</td>\n",
              "      <td>2243126.0</td>\n",
              "      <td>2296752.0</td>\n",
              "      <td>2350124.0</td>\n",
              "      <td>2404831.0</td>\n",
              "      <td>2458526.0</td>\n",
              "      <td>2513546.0</td>\n",
              "      <td>2566266.0</td>\n",
              "      <td>2617832.0</td>\n",
              "      <td>2671997.0</td>\n",
              "      <td>2726056.0</td>\n",
              "      <td>2784278.0</td>\n",
              "      <td>2843960.0</td>\n",
              "      <td>2904429.0</td>\n",
              "      <td>2964762.0</td>\n",
              "      <td>3022635.0</td>\n",
              "      <td>3083605.0</td>\n",
              "      <td>3142336.0</td>\n",
              "      <td>3227943.0</td>\n",
              "      <td>3286542.0</td>\n",
              "      <td>3266790.0</td>\n",
              "      <td>3247039.0</td>\n",
              "      <td>3227287.0</td>\n",
              "      <td>3207536.0</td>\n",
              "      <td>3187784.0</td>\n",
              "      <td>3168033.0</td>\n",
              "      <td>3148281.0</td>\n",
              "      <td>3128530.0</td>\n",
              "      <td>3108778.0</td>\n",
              "      <td>3089027.0</td>\n",
              "      <td>3060173.0</td>\n",
              "      <td>3051010.0</td>\n",
              "      <td>3039616.0</td>\n",
              "      <td>3026939.0</td>\n",
              "      <td>3011487.0</td>\n",
              "      <td>2992547.0</td>\n",
              "      <td>2970017.0</td>\n",
              "      <td>2947314.0</td>\n",
              "      <td>2927519.0</td>\n",
              "      <td>2913021.0</td>\n",
              "      <td>2905195.0</td>\n",
              "      <td>2900401.0</td>\n",
              "      <td>2895092.0</td>\n",
              "      <td>2889104.0</td>\n",
              "      <td>2880703.0</td>\n",
              "      <td>2876101.0</td>\n",
              "      <td>2873457.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AND</td>\n",
              "      <td>13411.0</td>\n",
              "      <td>14375.0</td>\n",
              "      <td>15370.0</td>\n",
              "      <td>16412.0</td>\n",
              "      <td>17469.0</td>\n",
              "      <td>18549.0</td>\n",
              "      <td>19647.0</td>\n",
              "      <td>20758.0</td>\n",
              "      <td>21890.0</td>\n",
              "      <td>23058.0</td>\n",
              "      <td>24276.0</td>\n",
              "      <td>25559.0</td>\n",
              "      <td>26892.0</td>\n",
              "      <td>28232.0</td>\n",
              "      <td>29520.0</td>\n",
              "      <td>30705.0</td>\n",
              "      <td>31777.0</td>\n",
              "      <td>32771.0</td>\n",
              "      <td>33737.0</td>\n",
              "      <td>34818.0</td>\n",
              "      <td>36067.0</td>\n",
              "      <td>37500.0</td>\n",
              "      <td>39114.0</td>\n",
              "      <td>40867.0</td>\n",
              "      <td>42706.0</td>\n",
              "      <td>44600.0</td>\n",
              "      <td>46517.0</td>\n",
              "      <td>48455.0</td>\n",
              "      <td>50434.0</td>\n",
              "      <td>52448.0</td>\n",
              "      <td>54509.0</td>\n",
              "      <td>56671.0</td>\n",
              "      <td>58888.0</td>\n",
              "      <td>60971.0</td>\n",
              "      <td>62677.0</td>\n",
              "      <td>63850.0</td>\n",
              "      <td>64360.0</td>\n",
              "      <td>64327.0</td>\n",
              "      <td>64142.0</td>\n",
              "      <td>64370.0</td>\n",
              "      <td>65390.0</td>\n",
              "      <td>67341.0</td>\n",
              "      <td>70049.0</td>\n",
              "      <td>73182.0</td>\n",
              "      <td>76244.0</td>\n",
              "      <td>78867.0</td>\n",
              "      <td>80991.0</td>\n",
              "      <td>82683.0</td>\n",
              "      <td>83861.0</td>\n",
              "      <td>84462.0</td>\n",
              "      <td>84449.0</td>\n",
              "      <td>83751.0</td>\n",
              "      <td>82431.0</td>\n",
              "      <td>80788.0</td>\n",
              "      <td>79223.0</td>\n",
              "      <td>78014.0</td>\n",
              "      <td>77281.0</td>\n",
              "      <td>76965.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Country Code       1960       1961  ...        2015        2016        2017\n",
              "0          ABW    54211.0    55438.0  ...    104341.0    104822.0    105264.0\n",
              "1          AFG  8996351.0  9166764.0  ...  33736494.0  34656032.0  35530081.0\n",
              "2          AGO  5643182.0  5753024.0  ...  27859305.0  28813463.0  29784193.0\n",
              "3          ALB  1608800.0  1659800.0  ...   2880703.0   2876101.0   2873457.0\n",
              "4          AND    13411.0    14375.0  ...     78014.0     77281.0     76965.0\n",
              "\n",
              "[5 rows x 59 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlOdEuYn39hj",
        "colab_type": "text"
      },
      "source": [
        "## Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzmjXIQP39hk",
        "colab_type": "text"
      },
      "source": [
        "### Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LOAHgBy39hl",
        "colab_type": "text"
      },
      "source": [
        "The world population data spans from 1960 to 2017. We'd like to build a predictive model that can give us the best guess at what the future population of a particular country might be. To do this, we're going to ignore the 2017 column from our data, and use this as a metric for testing the accuracy of our prediction.\n",
        "\n",
        "First, however, we need to formulate our data such that the sklearn's `LinearRegression` class can train on our data. To do this, we will write a function that takes as input a country code and return a 2-d numpy array that contains the year and the measured population. \n",
        "\n",
        "_**Function Specifications:**_\n",
        "* Should take a `str` as input and return a numpy `array` type as output.\n",
        "* The array should only have two columns containing the year and the population, in other words, it should have a shape `(?, 2)` where `?` is the length of the data.\n",
        "* The values within the array should be of type `int`.\n",
        "\n",
        "_**Hint:**_\n",
        "You'll first need to filter the dataframe on the given country code. Once you do this, you'll notice the DataFrame has the years as columns. You can just use `pd.melt(df)` and slice your data to get to the right format, before using `.get_values()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD6HiOEv39hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_year_pop(code):\n",
        "    \n",
        "    df = pd.DataFrame(df_pop)\n",
        "    #df = df.drop(['2017'],axis=1)\n",
        "    df =df[df['Country Code'] == code]\n",
        "    df = pd.melt(df, id_vars=['Country Code'], var_name='Year', value_name='Population')\n",
        "    df = df.drop(['Country Code'],axis=1) \n",
        "    df['Year'] = df['Year'].astype(int)\n",
        "    df['Population'] = df['Population'].astype(int)\n",
        "    df = df.get_values()\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F4XNv1639hp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb9d2426-e621-4e6d-cc06-6b3bb27f4f81"
      },
      "source": [
        "get_year_pop('ABW').shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTosSyRc39hr",
        "colab_type": "text"
      },
      "source": [
        "_**Expected Outputs:**_\n",
        "```python\n",
        "get_year_pop('ABW')\n",
        "```\n",
        "> ```\n",
        "array([[  1960,  54211],\n",
        "       [  1961,  55438],\n",
        "       [  1962,  56225],\n",
        "        ...\n",
        "       [  2016, 104822],\n",
        "       [  2017, 105264]])\n",
        "```\n",
        "\n",
        "```python\n",
        "get_year_pop('ABW').shape == (58, 2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV3ZzFcZ39hs",
        "colab_type": "text"
      },
      "source": [
        "### Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4kuDjsI39ht",
        "colab_type": "text"
      },
      "source": [
        "Now that we have have our data, we need to split this into a set of variables we will be training on, and the set of variables that we will make our predictions on. In this case, we're splitting the values such that we train on all but the last year in our dataset. We also need to split our data into the predictive features (denoted `X`) and the response (denoted `y`). \n",
        "\n",
        "Write a function that will take as input a 2-d numpy array and return four variables in the form of `(X_train, y_train), (X_test, y_test)`, where `(X_train, y_train)` are the features / response of the training set, and `(X-test, y_test)` are the feautes / response of the testing set.\n",
        "\n",
        "_**Function Specifications:**_\n",
        "* Should take a 2-d numpy `array` as input.\n",
        "* Should return two `tuples` of the form `(X_train, y_train), (X_test, y_test)`.\n",
        "* `(X_test, y_test)` should just be the last entry of the given input. They should also be the form of an `array`, and not as a single value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WBU20Uw39hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_response_split(arr):\n",
        "  \n",
        "    X_train = arr[0:-1,0]\n",
        "    y_train = arr[0:-1,1]\n",
        "    X_test =arr[-1:,0]\n",
        "    y_test =arr[-1:,1]\n",
        "    return (X_train, y_train), (X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDcd2K9C39hx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "478eea12-4d87-4302-d025-ad8a77e67cd2"
      },
      "source": [
        "data = get_year_pop('ABW')\n",
        "feature_response_split(data)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970,\n",
              "         1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981,\n",
              "         1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992,\n",
              "         1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,\n",
              "         2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014,\n",
              "         2015, 2016]),\n",
              "  array([ 54211,  55438,  56225,  56695,  57032,  57360,  57715,  58055,\n",
              "          58386,  58726,  59063,  59440,  59840,  60243,  60528,  60657,\n",
              "          60586,  60366,  60103,  59980,  60096,  60567,  61345,  62201,\n",
              "          62836,  63026,  62644,  61833,  61079,  61032,  62149,  64622,\n",
              "          68235,  72504,  76700,  80324,  83200,  85451,  87277,  89005,\n",
              "          90853,  92898,  94992,  97017,  98737, 100031, 100832, 101220,\n",
              "         101353, 101453, 101669, 102053, 102577, 103187, 103795, 104341,\n",
              "         104822])),\n",
              " (array([2017]), array([105264])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q0ysCQU39h1",
        "colab_type": "text"
      },
      "source": [
        "_**Expected Outputs:**_\n",
        "```python\n",
        "data = get_year_pop('ABW')\n",
        "feature_response_split(data)\n",
        "```\n",
        "> ```\n",
        "((array([1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970,\n",
        "         1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981,\n",
        "         1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992,\n",
        "         1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,\n",
        "         2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014,\n",
        "         2015, 2016]),\n",
        "  array([ 54211,  55438,  56225,  56695,  57032,  57360,  57715,  58055,\n",
        "          58386,  58726,  59063,  59440,  59840,  60243,  60528,  60657,\n",
        "          60586,  60366,  60103,  59980,  60096,  60567,  61345,  62201,\n",
        "          62836,  63026,  62644,  61833,  61079,  61032,  62149,  64622,\n",
        "          68235,  72504,  76700,  80324,  83200,  85451,  87277,  89005,\n",
        "          90853,  92898,  94992,  97017,  98737, 100031, 100832, 101220,\n",
        "         101353, 101453, 101669, 102053, 102577, 103187, 103795, 104341,\n",
        "         104822])),\n",
        " (array([2017]), array([105264])))\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-flImtLg39h2",
        "colab_type": "text"
      },
      "source": [
        "### Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqvb6a3v39h4",
        "colab_type": "text"
      },
      "source": [
        "Now that we have formatted our data, we can fit a model using sklearn's `LinearRegression()` class. We'll write a function that will take as input the features and response variables that we created in the last question, and return a trained model.\n",
        "\n",
        "_**Function Specifications:**_\n",
        "* Should take two numpy `arrays` as input in the form `(X_train, y_train)`.\n",
        "* Should return an sklearn `LinearRegression` model.\n",
        "* The returned model should be fitted to the data.\n",
        "\n",
        "_**Hint:**_\n",
        "You may need to reshape the data within the function. You can use `.reshape(-1, 1)` to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUCmbar439h5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(X_train, y_train):\n",
        "    from sklearn import linear_model\n",
        "    reg = linear_model.LinearRegression()\n",
        "    reg.fit(X_train.reshape(-1, 1),y_train.reshape(-1, 1))\n",
        "    return reg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK7O1sNi39h7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1778b32-4c58-4c05-ff56-74275f48d7f8"
      },
      "source": [
        "data = get_year_pop('ABW')\n",
        "(X_train, y_train), _ = feature_response_split(data)\n",
        "\n",
        "train_model(X_train, y_train).predict([[2017]])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[104770.18984962]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY5jHLkn39h-",
        "colab_type": "text"
      },
      "source": [
        "_**Expected Outputs:**_\n",
        "```python\n",
        "train_model(X_train, y_train).predict([[2017]]) == array([[104770.18984962]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRGLYZ-S39h_",
        "colab_type": "text"
      },
      "source": [
        "### Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktka0bnl39iA",
        "colab_type": "text"
      },
      "source": [
        "We would now like to test our model using the testing data that we produced from Question 2. This test should give the residual sum of squares, which for your convenience is written as\n",
        "$$\n",
        "RSS = \\sum_{i=1}^N (p_i - y_i)^2,\n",
        "$$\n",
        "where $p_i$ refers to the $i^{\\rm th}$ prediction made from `X_test`, $y_i$ refers to the $i^{\\rm th}$ value in `y_test`, and $N$ is the length of `y_test`.\n",
        "\n",
        "_**Function Specifications:**_\n",
        "* Should take a trained model and two `arrays` as input. This will be the `X_test` and `y_test` variables from Question 2. \n",
        "* Should return the residual sum of squares over the input from the predicted values of `X_test` as compared to values of `y_test`.\n",
        "* The output should be a `float` rounded to 2 decimal places."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlGosTKY39iB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(model, X_test, y_test):\n",
        "    \n",
        "    return np.round(((sum(model.predict(X_test.reshape(-1, 1)) - y_test.reshape(-1, 1)))**2),2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlXy_wOF39iD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41885a29-d202-42e6-fb68-54440d806395"
      },
      "source": [
        "data = get_year_pop('ABW')\n",
        "(X_train, y_train), (X_test, y_test) = feature_response_split(data)\n",
        "lm = train_model(X_train, y_train)\n",
        "\n",
        "test_model(lm, X_test, y_test)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([243848.46])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqy5DzKo39iG",
        "colab_type": "text"
      },
      "source": [
        "_**Expected Outputs:**_\n",
        "```python\n",
        "test_model(lm, X_test, y_test) == 243848.46\n",
        "```"
      ]
    }
  ]
}